{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Poker Bot RL Training - Overnight Run (8 Hours)\n",
    "\n",
    "This notebook trains your ensemble agents using Actor-Critic reinforcement learning.\n",
    "\n",
    "**Estimated training time:** 7-8 hours on Colab GPU  \n",
    "**Expected hands:** 500K-800K  \n",
    "**Expected improvement:** +10-15% win rate\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Upload to Google Colab**\n",
    "2. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
    "3. **Run all cells**\n",
    "4. **Let it train overnight**\n",
    "5. **Download trained models in the morning**\n",
    "\n",
    "The notebook will save checkpoints every 10K hands, so you won't lose progress if it disconnects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n‚úÖ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone repository (or upload files manually)\n",
    "import os\n",
    "\n",
    "if not os.path.exists('AI-Texas-Holdem-CSC4444'):\n",
    "    !git clone https://github.com/vekoLSU/AI-Texas-Holdem-CSC4444.git\n",
    "    %cd AI-Texas-Holdem-CSC4444\n",
    "else:\n",
    "    %cd AI-Texas-Holdem-CSC4444\n",
    "    !git pull\n",
    "\n",
    "print(\"‚úÖ Repository ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy websockets tqdm\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import and Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Import poker bot components\n",
    "from poker_bot.training.networks import ActorCriticAgent\n",
    "from poker_bot.evaluation import HandEvaluator\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Simplified Poker Environment for Self-Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimplePokerEnv:\n",
    "    \"\"\"Simplified poker environment for self-play training.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hand_evaluator = HandEvaluator()\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment for new hand.\"\"\"\n",
    "        # 2 players, heads-up\n",
    "        self.players = [{'chips': 1000, 'bet': 0, 'folded': False} for _ in range(2)]\n",
    "        self.pot = 0\n",
    "        self.current_bet = 10  # Big blind\n",
    "        self.phase = 'PREFLOP'\n",
    "        self.community_cards = []\n",
    "        \n",
    "        # Deal cards (simplified)\n",
    "        deck = self._create_deck()\n",
    "        random.shuffle(deck)\n",
    "        self.player_cards = [deck[:2], deck[2:4]]\n",
    "        self.community_cards = deck[4:9]  # Will reveal progressively\n",
    "        \n",
    "        return self._get_state(0)\n",
    "    \n",
    "    def _create_deck(self):\n",
    "        \"\"\"Create standard 52-card deck.\"\"\"\n",
    "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A']\n",
    "        suits = ['h', 'd', 'c', 's']\n",
    "        return [r + s for r in ranks for s in suits]\n",
    "    \n",
    "    def _get_state(self, player_id, full_info=False):\n",
    "        \"\"\"Get state representation.\"\"\"\n",
    "        # Simplified state features (50 dims for actor, 70 for critic)\n",
    "        state = []\n",
    "        \n",
    "        # Hand strength\n",
    "        visible_community = self._get_visible_community()\n",
    "        hand_eval = self.hand_evaluator.evaluate_hand_strength(\n",
    "            self.player_cards[player_id],\n",
    "            visible_community,\n",
    "            self.phase\n",
    "        )\n",
    "        state.append(hand_eval.get('strength', 0.5))\n",
    "        \n",
    "        # Pot and bet info\n",
    "        state.append(self.pot / 2000)  # Normalized\n",
    "        state.append(self.current_bet / 1000)\n",
    "        state.append(self.players[player_id]['chips'] / 1000)\n",
    "        state.append(self.players[1-player_id]['chips'] / 1000)\n",
    "        \n",
    "        # Phase encoding (one-hot)\n",
    "        phases = ['PREFLOP', 'FLOP', 'TURN', 'RIVER']\n",
    "        phase_encoding = [1.0 if self.phase == p else 0.0 for p in phases]\n",
    "        state.extend(phase_encoding)\n",
    "        \n",
    "        # Pad to correct dimension\n",
    "        while len(state) < 50:\n",
    "            state.append(0.0)\n",
    "        \n",
    "        # For critic (full info), add opponent cards (20 dims more)\n",
    "        if full_info:\n",
    "            # Encode opponent hand (simplified)\n",
    "            opp_hand_eval = self.hand_evaluator.evaluate_hand_strength(\n",
    "                self.player_cards[1-player_id],\n",
    "                visible_community,\n",
    "                self.phase\n",
    "            )\n",
    "            state.extend([opp_hand_eval.get('strength', 0.5)] * 20)\n",
    "        \n",
    "        return np.array(state[:70 if full_info else 50], dtype=np.float32)\n",
    "    \n",
    "    def _get_visible_community(self):\n",
    "        \"\"\"Get visible community cards based on phase.\"\"\"\n",
    "        if self.phase == 'PREFLOP':\n",
    "            return []\n",
    "        elif self.phase == 'FLOP':\n",
    "            return self.community_cards[:3]\n",
    "        elif self.phase == 'TURN':\n",
    "            return self.community_cards[:4]\n",
    "        else:  # RIVER\n",
    "            return self.community_cards[:5]\n",
    "    \n",
    "    def step(self, player_id, action, amount_ratio):\n",
    "        \"\"\"Execute action and return next state, reward, done.\"\"\"\n",
    "        # Action: 0=fold, 1=call, 2=check, 3=raise\n",
    "        \n",
    "        if action == 0:  # Fold\n",
    "            self.players[player_id]['folded'] = True\n",
    "            reward = -self.players[player_id]['bet']  # Lost chips in pot\n",
    "            return None, reward, True\n",
    "        \n",
    "        elif action == 1:  # Call\n",
    "            to_call = self.current_bet - self.players[player_id]['bet']\n",
    "            self.players[player_id]['bet'] += to_call\n",
    "            self.players[player_id]['chips'] -= to_call\n",
    "            self.pot += to_call\n",
    "        \n",
    "        elif action == 2:  # Check\n",
    "            pass  # No chips change\n",
    "        \n",
    "        elif action == 3:  # Raise\n",
    "            raise_amount = int(amount_ratio * self.pot + self.current_bet)\n",
    "            raise_amount = min(raise_amount, self.players[player_id]['chips'])\n",
    "            raise_amount = max(raise_amount, self.current_bet + 10)  # Minimum raise\n",
    "            \n",
    "            to_add = raise_amount - self.players[player_id]['bet']\n",
    "            self.players[player_id]['bet'] += to_add\n",
    "            self.players[player_id]['chips'] -= to_add\n",
    "            self.pot += to_add\n",
    "            self.current_bet = raise_amount\n",
    "        \n",
    "        # Check if hand is over\n",
    "        done, reward = self._check_hand_end(player_id)\n",
    "        \n",
    "        if not done:\n",
    "            # Advance phase if betting round complete\n",
    "            if self._betting_round_complete():\n",
    "                self._advance_phase()\n",
    "                done, reward = self._check_hand_end(player_id)\n",
    "        \n",
    "        next_state = self._get_state(player_id) if not done else None\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def _betting_round_complete(self):\n",
    "        \"\"\"Check if betting round is complete.\"\"\"\n",
    "        # Simplified: assume complete if bets are equal\n",
    "        return self.players[0]['bet'] == self.players[1]['bet']\n",
    "    \n",
    "    def _advance_phase(self):\n",
    "        \"\"\"Move to next phase.\"\"\"\n",
    "        phases = ['PREFLOP', 'FLOP', 'TURN', 'RIVER', 'SHOWDOWN']\n",
    "        idx = phases.index(self.phase)\n",
    "        if idx < len(phases) - 1:\n",
    "            self.phase = phases[idx + 1]\n",
    "            # Reset bets for new round\n",
    "            self.players[0]['bet'] = 0\n",
    "            self.players[1]['bet'] = 0\n",
    "            self.current_bet = 0\n",
    "    \n",
    "    def _check_hand_end(self, player_id):\n",
    "        \"\"\"Check if hand is over and calculate reward.\"\"\"\n",
    "        # Someone folded\n",
    "        if self.players[1-player_id]['folded']:\n",
    "            return True, self.pot\n",
    "        \n",
    "        # Showdown\n",
    "        if self.phase == 'SHOWDOWN':\n",
    "            winner = self._determine_winner()\n",
    "            reward = self.pot if winner == player_id else -self.players[player_id]['bet']\n",
    "            return True, reward\n",
    "        \n",
    "        return False, 0\n",
    "    \n",
    "    def _determine_winner(self):\n",
    "        \"\"\"Determine winner at showdown.\"\"\"\n",
    "        strength_0 = self.hand_evaluator.evaluate_hand_strength(\n",
    "            self.player_cards[0], self.community_cards[:5], 'RIVER'\n",
    "        )['strength']\n",
    "        strength_1 = self.hand_evaluator.evaluate_hand_strength(\n",
    "            self.player_cards[1], self.community_cards[:5], 'RIVER'\n",
    "        )['strength']\n",
    "        return 0 if strength_0 > strength_1 else 1\n",
    "\n",
    "print(\"‚úÖ Poker environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_agent_selfplay(num_hands=500000, save_every=10000, checkpoint_dir='checkpoints'):\n",
    "    \"\"\"Train agent via self-play using PPO.\"\"\"\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = ActorCriticAgent(actor_state_dim=50, critic_state_dim=70).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n",
    "    \n",
    "    # Training params\n",
    "    gamma = 0.99  # Discount factor\n",
    "    clip_epsilon = 0.2  # PPO clip parameter\n",
    "    \n",
    "    # Stats tracking\n",
    "    episode_rewards = deque(maxlen=1000)\n",
    "    win_rate = deque(maxlen=1000)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    env = SimplePokerEnv()\n",
    "    \n",
    "    pbar = tqdm(total=num_hands, desc=\"Training\")\n",
    "    \n",
    "    for hand in range(num_hands):\n",
    "        # Play hand\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Storage for episode\n",
    "        states, actions, amounts, rewards, values, log_probs = [], [], [], [], [], []\n",
    "        \n",
    "        player_id = 0  # Training player 0\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get state tensors\n",
    "            actor_state = torch.FloatTensor(state[:50]).unsqueeze(0).to(device)\n",
    "            critic_state = torch.FloatTensor(env._get_state(player_id, full_info=True)).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get action from policy\n",
    "            with torch.no_grad():\n",
    "                action_probs, amount, value = agent(actor_state, critic_state)\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "            \n",
    "            action_idx = action.item()\n",
    "            amount_val = amount.item()\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(player_id, action_idx, amount_val)\n",
    "            \n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(action_idx)\n",
    "            amounts.append(amount_val)\n",
    "            rewards.append(reward)\n",
    "            values.append(value.item())\n",
    "            log_probs.append(log_prob.item())\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Alternate players (simplified - train only one agent)\n",
    "            player_id = 1 - player_id\n",
    "        \n",
    "        # Update policy\n",
    "        if len(rewards) > 0:\n",
    "            # Calculate returns\n",
    "            returns = []\n",
    "            R = 0\n",
    "            for r in reversed(rewards):\n",
    "                R = r + gamma * R\n",
    "                returns.insert(0, R)\n",
    "            \n",
    "            returns = torch.FloatTensor(returns).to(device)\n",
    "            values_tensor = torch.FloatTensor(values).to(device)\n",
    "            \n",
    "            # Advantages\n",
    "            advantages = returns - values_tensor\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # PPO update\n",
    "            for _ in range(4):  # Multiple epochs\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Recalculate log probs and values\n",
    "                batch_actor_states = torch.FloatTensor([s[:50] for s in states]).to(device)\n",
    "                batch_critic_states = torch.FloatTensor([env._get_state(0, full_info=True) for _ in states]).to(device)\n",
    "                \n",
    "                action_probs, amounts_pred, values_new = agent(batch_actor_states, batch_critic_states)\n",
    "                \n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                actions_tensor = torch.LongTensor(actions).to(device)\n",
    "                log_probs_new = dist.log_prob(actions_tensor)\n",
    "                \n",
    "                # PPO loss\n",
    "                ratio = torch.exp(log_probs_new - torch.FloatTensor(log_probs).to(device))\n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "                \n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = F.mse_loss(values_new.squeeze(), returns)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Track stats\n",
    "        episode_rewards.append(episode_reward)\n",
    "        win_rate.append(1.0 if episode_reward > 0 else 0.0)\n",
    "        \n",
    "        # Update progress\n",
    "        pbar.update(1)\n",
    "        if hand % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards)\n",
    "            avg_win_rate = np.mean(win_rate)\n",
    "            pbar.set_postfix({\n",
    "                'reward': f'{avg_reward:.1f}',\n",
    "                'win_rate': f'{avg_win_rate:.2%}',\n",
    "                'elapsed': f'{(time.time() - start_time) / 3600:.1f}h'\n",
    "            })\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (hand + 1) % save_every == 0:\n",
    "            checkpoint_path = f\"{checkpoint_dir}/agent_checkpoint_{hand+1}.pt\"\n",
    "            agent.save(checkpoint_path)\n",
    "            print(f\"\\n‚úÖ Checkpoint saved: {checkpoint_path}\")\n",
    "            \n",
    "            # Save training stats\n",
    "            stats = {\n",
    "                'hands': hand + 1,\n",
    "                'avg_reward': float(np.mean(episode_rewards)),\n",
    "                'win_rate': float(np.mean(win_rate)),\n",
    "                'elapsed_hours': (time.time() - start_time) / 3600\n",
    "            }\n",
    "            with open(f\"{checkpoint_dir}/stats_{hand+1}.json\", 'w') as f:\n",
    "                json.dump(stats, f, indent=2)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = f\"{checkpoint_dir}/agent_final.pt\"\n",
    "    agent.save(final_path)\n",
    "    print(f\"\\nüéâ Training complete! Final model saved: {final_path}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "print(\"‚úÖ Training function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start Training (Run Overnight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure training\n",
    "NUM_HANDS = 500000  # 500K hands (~7-8 hours on GPU)\n",
    "SAVE_EVERY = 10000  # Save checkpoint every 10K hands\n",
    "CHECKPOINT_DIR = 'trained_models'\n",
    "\n",
    "print(f\"üöÄ Starting training for {NUM_HANDS:,} hands\")\n",
    "print(f\"üíæ Checkpoints will be saved every {SAVE_EVERY:,} hands\")\n",
    "print(f\"üìÅ Output directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\n‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Start training\n",
    "trained_agent = train_agent_selfplay(\n",
    "    num_hands=NUM_HANDS,\n",
    "    save_every=SAVE_EVERY,\n",
    "    checkpoint_dir=CHECKPOINT_DIR\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Training finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüéâ Your trained models are ready to download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download final model and checkpoints\n",
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "print(\"üì• Downloading trained models...\")\n",
    "\n",
    "# Download final model\n",
    "files.download(f'{CHECKPOINT_DIR}/agent_final.pt')\n",
    "\n",
    "# Download latest checkpoint\n",
    "checkpoints = sorted(glob.glob(f'{CHECKPOINT_DIR}/agent_checkpoint_*.pt'))\n",
    "if checkpoints:\n",
    "    files.download(checkpoints[-1])\n",
    "\n",
    "# Download training stats\n",
    "stats_files = sorted(glob.glob(f'{CHECKPOINT_DIR}/stats_*.json'))\n",
    "if stats_files:\n",
    "    files.download(stats_files[-1])\n",
    "\n",
    "print(\"\\n‚úÖ Downloads complete!\")\n",
    "print(\"\\nüìã Next steps:\")\n",
    "print(\"1. Upload agent_final.pt to your project\")\n",
    "print(\"2. Load it into your ensemble agents\")\n",
    "print(\"3. Test against infrastructure\")\n",
    "print(\"4. DOMINATE THE COMPETITION! üèÜ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Load and plot training stats\n",
    "stats_files = sorted(glob.glob(f'{CHECKPOINT_DIR}/stats_*.json'))\n",
    "\n",
    "if stats_files:\n",
    "    hands_list = []\n",
    "    rewards_list = []\n",
    "    win_rates_list = []\n",
    "    \n",
    "    for stats_file in stats_files:\n",
    "        with open(stats_file) as f:\n",
    "            stats = json.load(f)\n",
    "            hands_list.append(stats['hands'])\n",
    "            rewards_list.append(stats['avg_reward'])\n",
    "            win_rates_list.append(stats['win_rate'])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(hands_list, rewards_list, linewidth=2)\n",
    "    ax1.set_xlabel('Hands Played')\n",
    "    ax1.set_ylabel('Average Reward')\n",
    "    ax1.set_title('Training Progress: Rewards')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot win rate\n",
    "    ax2.plot(hands_list, [w * 100 for w in win_rates_list], linewidth=2, color='green')\n",
    "    ax2.set_xlabel('Hands Played')\n",
    "    ax2.set_ylabel('Win Rate (%)')\n",
    "    ax2.set_title('Training Progress: Win Rate')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/training_progress.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    files.download(f'{CHECKPOINT_DIR}/training_progress.png')\n",
    "    print(\"‚úÖ Training visualization saved and downloaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training stats found. Run training first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
